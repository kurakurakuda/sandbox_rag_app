{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\users\\kris3\\appdata\\roaming\\python\\python39\\site-packages (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kris3\\appdata\\roaming\\python\\python39\\site-packages (4.11.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\kris3\\appdata\\roaming\\python\\python39\\site-packages (from requests) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kris3\\appdata\\roaming\\python\\python39\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kris3\\appdata\\roaming\\python\\python39\\site-packages (from requests) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kris3\\appdata\\roaming\\python\\python39\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kris3\\appdata\\roaming\\python\\python39\\site-packages (from beautifulsoup4) (2.3.2.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, csv\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    def scrape_links_by_class(self, urls: list, target_class: str) -> dict:\n",
    "        result = {}\n",
    "        for url in urls:\n",
    "            soup = self.__get_url(url)\n",
    "            if soup == None:\n",
    "                print(f'[scrape_links_by_class] このURLで取得結果なし: {url}')\n",
    "                continue\n",
    "            \n",
    "            # タイトルを取得\n",
    "            title = soup.title.string\n",
    "            print(f'[scrape_links_by_class] ページのタイトル: {title}')\n",
    "\n",
    "            # ドメインを取得\n",
    "            domain = self.__extract_host(url)\n",
    "            \n",
    "            # 目標のclass属性を持つul要素をすべて取得\n",
    "            target_ul = soup.find_all('ul', class_=target_class)\n",
    "\n",
    "            # 各ul内のリンクを取得\n",
    "            for ul in target_ul:\n",
    "                # ul内のリンクを取得\n",
    "                content = ul.find_all('a')\n",
    "            \n",
    "                # リンクのテキストとURLを表示\n",
    "                for link in content:\n",
    "                    link_text = link.text\n",
    "                    link_url = link.get('href')\n",
    "                    if link_url and link_text:\n",
    "                        result[link_text] = domain + link_url\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def scraped_text_by_id(self, url:str, id:str) -> str:\n",
    "            soup = self.__get_url(url)\n",
    "            if soup == None:\n",
    "                print(f'[scraped_content_by_id] このURLで取得結果なし: {url}')\n",
    "                return ''\n",
    "            \n",
    "            # タイトルを取得\n",
    "            title = soup.title.string\n",
    "            print(f'[scraped_content_by_id] ページのタイトル: {title}')\n",
    "            \n",
    "            # 目標のid属性を持つdiv要素のうち、最初のdiv要素を取得\n",
    "            target_div = soup.find('div', id=id)\n",
    "            clean_text = target_div.get_text()\n",
    "            clean_text = ' '.join(clean_text.split())\n",
    "            return title + ' ' + clean_text\n",
    "\n",
    "    def scraped_text_by_table(self, url:str) -> str:\n",
    "            soup = self.__get_url(url)\n",
    "            if soup == None:\n",
    "                print(f'[scraped_text_by_table] このURLで取得結果なし: {url}')\n",
    "                return ''\n",
    "            \n",
    "            # タイトルを取得\n",
    "            title = soup.title.string\n",
    "            print(f'[scraped_text_by_table] ページのタイトル: {title}')\n",
    "            \n",
    "            # table要素のうち、最初のtable要素を取得\n",
    "            target_table = soup.find('table')\n",
    "            clean_text = target_table.get_text()\n",
    "            clean_text = ' '.join(clean_text.split())\n",
    "            return title + ' ' + clean_text\n",
    "    \n",
    "    def __get_url(self, url: str) -> any:\n",
    "        # ウェブページのコンテンツを取得\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # ステータスコードが200 (成功) の場合にのみ処理を続行\n",
    "        if response.status_code == 200:\n",
    "            # Beautiful Soupを使用してHTMLを解析\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            return soup                   \n",
    "        else:\n",
    "            print(f'[__get_url] エラーコード: {response.status_code}')\n",
    "            return None\n",
    "        \n",
    "    def __extract_host(self, url: str) -> str:\n",
    "        url = '{uri.scheme}://{uri.netloc}'.format(uri=urlparse(url))\n",
    "        return url\n",
    "        \n",
    "scraper = Scraper()\n",
    "csv_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Pageのリンク一覧\n",
    "urls_blog = []\n",
    "\n",
    "# 目標のclass属性\n",
    "# 記事一覧のクラス\n",
    "target_class = 'contentsList'  \n",
    "\n",
    "# 目的のタイトル\n",
    "target_title = ''\n",
    "\n",
    "# 記事タイトルとリンクの保存先\n",
    "target_link_map, scraped_link_map = {}, {}\n",
    "scraped_link_map = scraper.scrape_links_by_class(urls_blog, target_class)\n",
    "\n",
    "for text, url in scraped_link_map.items():\n",
    "    if target_title in text:\n",
    "        target_link_map[text] = url\n",
    "\n",
    "print('対象リンク一覧 ---')\n",
    "for text, url in target_link_map.items():\n",
    "    print(f'リンクテキスト: {text}')\n",
    "    print(f'リンクURL: {url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ウェブページのコンテンツを取して、csv_dataに格納\n",
    "target_id = ''\n",
    "\n",
    "for title, url in target_link_map.items():\n",
    "    print(title, url)\n",
    "    text = scraper.scraped_text_by_id(url, target_id)\n",
    "    print(len(text), text)\n",
    "    csv_data.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "# csvに落とし込む\n",
    "csv_path = '../data/data.csv'\n",
    "print(len(csv_data))\n",
    "\n",
    "csv_raw_data = []\n",
    "\n",
    "for data in csv_data:\n",
    "    csv_raw_data.append([data, ])\n",
    "\n",
    "# CSVファイルへの書き込み\n",
    "with open(csv_path, mode='w', encoding='UTF-8', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(csv_raw_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
